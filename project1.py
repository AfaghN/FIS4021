# -*- coding: utf-8 -*-
"""project1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Qc8iEQXUUx_SP-IxuKDEsh759TORWWRS
"""

!pip install scikit-learn numpy matplotlib
!pip install mlxtend

"""با استفاده از datasets.sklearn، یک دیتاست با ۱۰۰۰ نمونه، ۲ کلاس و ۲ ویژگی تولید کنید."""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification, make_blobs, make_circles
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score , classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from mlxtend.plotting import plot_decision_regions



X, y = make_classification(
    n_samples=1000,
    n_features=2,
    n_redundant=0,
    n_classes=2,
    #class_sep=2,
    random_state=13
)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)

model=LogisticRegression(solver='saga',max_iter=200,random_state=13)
model.fit(X_train, y_train)
logistic_regression_pred = model.predict(X_test)
accuracy_model_predict_train=model.score(X_train,y_train)
logistic_regression_accuracy = accuracy_score(y_test, logistic_regression_pred)


print("Logistic Regression Classifier Accuracy (train):", accuracy_model_predict_train)
print("Logistic Regression Classifier Accuracy (test):", logistic_regression_accuracy)

sgd_classifier = SGDClassifier(loss='log_loss',random_state=31)
sgd_classifier.fit(X_train, y_train)
sgd_classifier_pred = sgd_classifier.predict(X_test)
accuracy_SGD_Classifier_train=model.score(X_train,y_train)
sgd_classifier_accuracy = accuracy_score(y_test, sgd_classifier_pred)

print("SGD Classifier Accuracy (train):", accuracy_model_predict_train)
print("SGD Classifier Accuracy (test):", sgd_classifier_accuracy)

colors=np.array(['blue','red'])
plt.scatter(X[:,0],X[:,1],c=colors[y])
plt.show()

all_predictions = model.predict(X)
incorrect_samples = np.where(all_predictions != y)[0]
plot_decision_regions(X, y, clf=model, markers='o', scatter_kwargs={'s': 50})
plt.title("Decision Boundary")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")

plt.figure()
plt.scatter(X[incorrect_samples, 0], X[incorrect_samples, 1], c='red', marker='x', s=200, linewidths=3)
plt.title("Incorrectly Classified Samples")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()

X2, y2 = make_classification(
    n_samples=1000,
    n_features=2,
    n_redundant=0,
    n_classes=2,
    class_sep=0.5,
    random_state=13
)
X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=13)

model2=LogisticRegression(solver='saga',max_iter=200,random_state=13)
model2.fit(X2_train, y2_train)
logistic_regression_pred2 = model2.predict(X2_test)
accuracy_model_predict_train2=model2.score(X2_train,y2_train)
logistic_regression_accuracy2 = accuracy_score(y2_test, logistic_regression_pred2)


print("Logistic Regression Classifier Accuracy (train):", accuracy_model_predict_train2)
print("Logistic Regression Classifier Accuracy (test):", logistic_regression_accuracy2)

sgd_classifier2 = SGDClassifier(loss='log_loss',random_state=31)
sgd_classifier2.fit(X2_train, y2_train)
sgd_classifier_pred2 = sgd_classifier2.predict(X2_test)
accuracy_SGD_Classifier_train2=model2.score(X2_train,y2_train)
sgd_classifier_accuracy2= accuracy_score(y2_test, sgd_classifier_pred2)

print("SGD Classifier Accuracy (train):", accuracy_model_predict_train2)
print("SGD Classifier Accuracy (test):", sgd_classifier_accuracy2)

colors=np.array(['green','orange'])
plt.scatter(X2[:,0],X2[:,1],c=colors[y2])
plt.show()

all_predictions2 = model2.predict(X2)
incorrect_samples2 = np.where(all_predictions2 != y2)[0]
plot_decision_regions(X2, y2, clf=model2, markers='o', scatter_kwargs={'s': 50})
plt.title("Decision Boundary")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")

plt.figure()
plt.scatter(X2[incorrect_samples2, 0], X2[incorrect_samples2, 1], c='red', marker='x', s=200, linewidths=3)
plt.title("Incorrectly Classified Samples")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()

X3, y3 = make_classification(
    n_samples=1000,
    n_features=2,
    n_redundant=0,
    n_classes=3,
    n_clusters_per_class=1,
    random_state=13
)

X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.2, random_state=13)
model3 = LogisticRegression(random_state=13, max_iter=1000)
model3.fit(X3_train, y3_train)

y3_pred = model3.predict(X3_test)
accuracy = accuracy_score(y3_test, y3_pred)

print("Accuracy:", accuracy)

all_predictions3 = model3.predict(X3)
incorrect_samples3 = np.where(all_predictions3 != y3)[0]
plot_decision_regions(X3, y3, clf=model3, markers='o', scatter_kwargs={'s': 50})

if incorrect_samples3 is not None:
    plt.scatter(X3[incorrect_samples3, 0], X3[incorrect_samples3, 1], c='red', marker='x', s=200, linewidths=3, label='Misclassified')

plt.title("Decision Boundaries and Data Points (3 Classes)")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.legend()
plt.show()

!pip install gdown
drive_link = "https://drive.google.com/uc?id=1c1oRtAv-szY_nOIPXLBi3k9beU3zmucI"
!gdown $drive_link -O dataset.csv

import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv('/content/dataset.csv', header=None)

df_shuffled = df.sample(frac=1, random_state=13)

train_ratio = 0.8
train_data, eval_data = train_test_split(df_shuffled, train_size=train_ratio, stratify=df_shuffled[4])
train_data_matrix = train_data.values
eval_data_matrix = eval_data.values

X_train_data = train_data_matrix[:, :4]
y_train_data = train_data_matrix[:, 4]

X_test_data = eval_data_matrix[:, :4]
y_test_data = eval_data_matrix[:, 4]

# چاپ ابعاد ماتریس‌ها
print(" trainابعاد ماتریس ویژگی‌ها (X_train_data):", X_train_data.shape)
print("trainابعاد ماتریس کلاس (y_train_data):", y_train_data.shape)
print("testابعاد ماتریس ویژگی‌ها (X_test_data):", X_test_data.shape)
print("testابعاد ماتریس کلاس (y_test_data):", y_test_data.shape)

import numpy as np
import matplotlib.pyplot as plt

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def binary_cross_entropy_loss(y_true, y_pred):
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)).mean()

def gradient(X, y, weights):
    predictions = sigmoid(np.dot(X, weights))
    error = predictions - y
    gradient = np.dot(X.T, error) / len(y)
    return gradient

def gradient_descent(X, y, learning_rate, epochs):
    weights = np.zeros(X.shape[1])
    losses = []

    for epoch in range(epochs):
        predictions = sigmoid(np.dot(X, weights))
        error = predictions - y
        weights -= learning_rate * gradient(X, y, weights)
        loss = binary_cross_entropy_loss(y, predictions)
        losses.append(loss)

    return weights, losses


X_train_data = np.column_stack([np.ones(len(X_train_data)), X_train_data])
X_test_data = np.column_stack([np.ones(len(X_test_data)), X_test_data])

learning_rate = 0.01
epochs = 1000
weights, losses = gradient_descent(X_train_data, y_train_data, learning_rate, epochs)

predictions_train = sigmoid(np.dot(X_train_data, weights))
predictions_test = sigmoid(np.dot(X_test_data, weights))


accuracy_train = np.mean((predictions_train >= 0.5).astype(int) == y_train_data)
accuracy_test = np.mean((predictions_test >= 0.5).astype(int) == y_test_data)

plt.plot(range(epochs), losses)
plt.title('Binary Cross-Entropy Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.show()

print(f"Training Accuracy: {accuracy_train}")
print(f"Testing Accuracy: {accuracy_test}")

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
normalized_data = scaler.fit_transform(df.iloc[:, :-1])
normalized_df = pd.DataFrame(normalized_data, columns=df.columns[:-1])
normalized_df[4] = df[4]

normalized_df_shuffled = normalized_df.sample(frac=1, random_state=13)

train_ratio = 0.8
train_data, eval_data = train_test_split(normalized_df_shuffled, train_size=train_ratio, stratify=normalized_df_shuffled[4])
train_data_matrix = train_data.values
eval_data_matrix = eval_data.values

X_train_data = train_data_matrix[:, :4]
y_train_data = train_data_matrix[:, 4]

X_test_data = eval_data_matrix[:, :4]
y_test_data = eval_data_matrix[:, 4]

X_train_data = np.column_stack([np.ones(len(X_train_data)), X_train_data])
X_test_data = np.column_stack([np.ones(len(X_test_data)), X_test_data])

learning_rate = 0.01
epochs = 1000
weights, losses = gradient_descent(X_train_data, y_train_data, learning_rate, epochs)

predictions_train = sigmoid(np.dot(X_train_data, weights))
predictions_test = sigmoid(np.dot(X_test_data, weights))


accuracy_train = np.mean((predictions_train >= 0.5).astype(int) == y_train_data)
accuracy_test = np.mean((predictions_test >= 0.5).astype(int) == y_test_data)

plt.plot(range(epochs), losses)
plt.title('Binary Cross-Entropy Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.show()

print(f"Training Accuracy: {accuracy_train}")
print(f"Testing Accuracy: {accuracy_test}")

import matplotlib.pyplot as plt

df.colums=['variance','skewness','curtosis','entropy','class']
class_counts = df['class'].value_counts()
plt.bar(class_counts.index, class_counts.values, color=['blue', 'orange'])
plt.xlabel('Class')
plt.ylabel('Number of Samples')
plt.title('Class Distribution in the Dataset')
plt.xticks(class_counts.index, ['Class 0', 'Class 1'])
plt.show()

pip install -U imbalanced-learn

from imblearn.over_sampling import RandomOverSampler
print(df.columns)

!pip install --upgrade --no-cache-dir gdown
#"https://drive.google.com/file/d/1lKh4UnnR1wmo271ddfAi5VtRdMyqP1vB/view?usp=sharing"
!gdown 1lKh4UnnR1wmo271ddfAi5VtRdMyqP1vB

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

df = pd.read_csv('/content/heart_disease_health_indicators.csv')

class_1_samples = df[df['HeartDiseaseorAttack'] == 1].sample(100, random_state=13)

class_0_samples = df[df['HeartDiseaseorAttack'] == 0].sample(100, random_state=13)

new_df = pd.concat([class_1_samples, class_0_samples], ignore_index=True)
y=new_df[['HeartDiseaseorAttack']].values
X = new_df[['HighBP', 'HighChol', 'CholCheck', 'BMI', 'Smoker', 'Stroke', 'Diabetes', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age','Education','Income']].values
#print(new_df.shape)
#print(X.shape)
#X

from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.metrics import accuracy_score , classification_report
from sklearn.preprocessing import StandardScaler
from mlxtend.plotting import plot_decision_regions


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)
y_train = y_train.ravel()
y_test = y_test.ravel()


model = LogisticRegression(max_iter=1000, random_state=13)
model.fit(X_train, y_train)
logistic_regression_pred = model.predict(X_test)
accuracy_model_predict_train=model.score(X_train,y_train)
logistic_regression_accuracy = accuracy_score(y_test, logistic_regression_pred)


print("Logistic Regression Classifier Accuracy (train):", accuracy_model_predict_train)
print("Logistic Regression Classifier Accuracy (test):", logistic_regression_accuracy)

sgd_classifier = SGDClassifier(loss='log_loss',random_state=31)
sgd_classifier.fit(X_train, y_train)
sgd_classifier_pred = sgd_classifier.predict(X_test)
accuracy_SGD_Classifier_train=model.score(X_train,y_train)
sgd_classifier_accuracy = accuracy_score(y_test, sgd_classifier_pred)

print("SGD Classifier Accuracy (train):", accuracy_model_predict_train)
print("SGD Classifier Accuracy (test):", sgd_classifier_accuracy)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import log_loss

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

model = SGDClassifier(loss='log_loss', max_iter=1000, random_state=13)
model.fit(X_train_scaled, y_train)

loss_history = []

for epoch in range(1, model.max_iter + 1):
    model.partial_fit(X_train_scaled, y_train, classes=np.unique(y_train))
    loss = log_loss(y_train, model.predict_proba(X_train_scaled)[:, 1])
    loss_history.append(loss)

plt.plot(loss_history)
plt.title('Loss Curve')
plt.xlabel('Number of Iterations')
plt.ylabel('Log Loss')
plt.show()

from sklearn.metrics import precision_score, recall_score, f1_score

precision = precision_score(y_test, logistic_regression_pred)
recall = recall_score(y_test, logistic_regression_pred)

f1 = f1_score(y_test, logistic_regression_pred)

print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)